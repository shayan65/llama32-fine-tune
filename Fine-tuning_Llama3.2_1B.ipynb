{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "317e662c",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama 3.2 1B Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a117bf99",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to fine-tune the Llama 3.2 1B model using Hugging Face, PEFT, and LoRA for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34603c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers==4.44.2\n",
    "!pip install datasets\n",
    "!pip install accelerate\n",
    "!pip install peft\n",
    "!pip install bitsandbytes\n",
    "!pip install wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be42a62d",
   "metadata": {},
   "source": [
    "## 1. Load the Pre-trained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de126539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc4289",
   "metadata": {},
   "source": [
    "## 2. Prepare Dataset for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4731dc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a sample dataset for fine-tuning\n",
    "dataset = load_dataset(\"your_dataset_name\")  # Replace with your dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb4dded",
   "metadata": {},
   "source": [
    "## 3. Set Up the Trainer and Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d829d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cf264a",
   "metadata": {},
   "source": [
    "## 4. Fine-tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56c4147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Begin training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39530201",
   "metadata": {},
   "source": [
    "## 5. Save the Fine-tuned Model and Push to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3654b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model locally\n",
    "model.save_pretrained(\"./fine-tuned-model\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-model\")\n",
    "\n",
    "# Optionally, push to Hugging Face Hub\n",
    "model.push_to_hub(\"your_hf_model_name\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be9040",
   "metadata": {},
   "source": [
    "## 6. Run Inference with Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc036b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample inference\n",
    "inputs = tokenizer(\"Your test input\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
